# -*- coding: utf-8 -*-
"""Exp-3-LogisticRegression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BhN_6qumzVSPVGIy7R6QmabZCCeHUH_i
"""

import pandas as pd

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
import seaborn as sns

df = pd.read_csv('/content/heart_disease_detection.csv')

df.head()

df.info()

sns.countplot(df,x='TenYearCHD')

df.describe()

df = df.dropna()

df.shape

# X Data
X = df.iloc[:, :-1]
# y Data
y = df.iloc[:, -1]

# Create histograms for all numerical columns
df.select_dtypes(include='number').hist(figsize=(12, 8))
plt.show()

df.corr()

df.isnull().sum()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(df.drop(columns = 'TenYearCHD'),df['TenYearCHD'], test_size=0.2, random_state=42 )

from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

imputer = SimpleImputer(strategy='mean')
ct = ColumnTransformer(
    transformers=[
        ('tnf1', imputer,['cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'glucose'])
    ],
    remainder='passthrough'
)

newx_train = ct.fit_transform(x_train)
newx_test = ct.transform(x_test)

clf = LogisticRegression()
clf2 = DecisionTreeClassifier()

clf.fit(newx_train,y_train)
clf2.fit(newx_train,y_train)

y_pred = clf.predict(newx_test)
y_pred1 = clf2.predict(newx_test)

print("Accuracy LR",accuracy_score(y_test,y_pred))
print("Accuracy DT",accuracy_score(y_test,y_pred1))

trf = FunctionTransformer(func=np.log1p)
X_train_transformed = trf.fit_transform(newx_train)
X_test_transformed = trf.transform(newx_test)
clf = LogisticRegression(max_iter=1000)
clf2 = DecisionTreeClassifier()
clf3 = KNeighborsClassifier(n_neighbors=5)
clf4 = RandomForestClassifier(n_estimators=100)

clf.fit(X_train_transformed,y_train)
clf2.fit(X_train_transformed,y_train)
clf3.fit(X_train_transformed, y_train)
clf4.fit(X_train_transformed, y_train)

y_pred = clf.predict(X_test_transformed)
y_pred1 = clf2.predict(X_test_transformed)
y_pred2 = clf3.predict(X_test_transformed)
y_pred3 = clf4.predict(X_test_transformed)

print("Accuracy LR",accuracy_score(y_test,y_pred))
print("Accuracy DT",accuracy_score(y_test,y_pred1))
print("Accuracy K-NN:", accuracy_score(y_test, y_pred2))
print("Accuracy Random Forest:", accuracy_score(y_test, y_pred3))

import scipy.stats as stats
columns_to_plot = ["cigsPerDay", "BPMeds", "age"]

for col in x_train.columns:
    if col in columns_to_plot:
        plt.figure(figsize=(14,4))
        plt.subplot(121)
        sns.histplot(x_train[col])
        plt.title(col)

        plt.subplot(122)
        stats.probplot(x_train[col], dist="norm", plot=plt)
        plt.title(col)

        plt.show()
